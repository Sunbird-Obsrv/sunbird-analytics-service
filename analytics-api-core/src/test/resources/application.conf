# This is the main configuration file for the application.
# ~~~~~

# Secret key
# ~~~~~
# The secret key is used to secure cryptographics functions.
# If you deploy your application to several instances be sure to use the same key!
application.secret="DFHdR74?tDvH2n`DAqk:RBl6NfkoO5tNYblTRmf3ZLcDIp5@oVjJM^ypkDOf2`<A"

# The application languages
# ~~~~~
application.langs="en"

# Global object class
# ~~~~~
# Define the Global object class for this application.
# Default to Global in the root package.
# application.global=Global

# Router
# ~~~~~
# Define the Router object to use for this application.
# This router will be looked up first when the application is starting up,
# so make sure this is the entry point.
# Furthermore, it's assumed your route file is named properly.
# So for an application router like `my.application.Router`,
# you may need to define a router file `conf/my.application.routes`.
# Default to Routes in the root package (and conf/routes)
# application.router=my.application.Routes

# Database configuration
# ~~~~~
# You can declare as many datasources as you want.
# By convention, the default datasource is named `default`
#
# db.default.driver=org.h2.Driver
# db.default.url="jdbc:h2:mem:play"
# db.default.user=sa
# db.default.password=""

# Evolutions
# ~~~~~
# You can disable evolutions if needed
# evolutionplugin=disabled

# Logger
# ~~~~~
# You can also configure logback (http://logback.qos.ch/),
# by providing an application-logger.xml file in the conf directory.

# Root logger:
logger.root=ERROR

# Logger used by the framework:
logger.play=INFO

# Logger provided to your application:
logger.application=DEBUG

# Test Configurations
cassandra.service.embedded.enable=true
cassandra.cql_path="../database/data.cql"
cassandra.service.embedded.connection.port=9142
cassandra.keyspace_prefix="local_"

spark.cassandra.connection.host="127.0.0.1"
spark.cassandra.connection.port=9142
application.env="local"

# Content to vec configurations
content2vec.content_service_url="https://dev.ekstep.in/api/learning"
content2vec.scripts_path="src/test/resources/python/main/vidyavaani"
content2vec.enrich_content="true"
content2vec.train_model="false"
content2vec.content_corpus="true"
content2vec.infer_query="true"
content2vec.s3_bucket="ekstep-dev-data-store"
content2vec.s3_key_prefix="DataSciences/model/"
content2vec.model_path="/tmp/content2vec/model/"
content2vec.kafka_topic="sandbox.learning.graph.events"
content2vec.kafka_broker_list="localhost:9092"
content2vec.infer_all="false"
content2vec.corpus_path="/tmp/content2vec/content_corpus/"
content2vec.download_path="/tmp/content2vec/download/"
content2vec.download_file_prefix="temp_"
content2vec.train_model_job="../../platform-scripts/shell/local/run-job.sh ctv &"
recommendation.train_model_job="../../platform-scripts/shell/local/run-job.sh device-recos &"


# Recommendations configurations
service.search.url="https://dev.ekstep.in/api/search"
service.search.path="/v2/search"
service.search.requestbody="""{"request":{"filters":{"objectType":["Content"],"contentType":["Story","Worksheet","Collection","Game"],"status":["Live"]},"limit":1000}}"""
service.search.limit="10"

recommendation.enable=true
recommendation.limit="10"
recommendation.surprise_find.enable=true
recommendation.surprise_find.index="15"
recommendation.surprise_find.serendipity_factor="20"

dataproduct.scripts_path=../../platform-scripts/shell/local
data.cql_path=../../platform-scripts/database/data.cql

# Metrics API configuration
#metrics.search.type="s3"
#metrics.search.params={"bucket":"ekstep-dev-data-store", "path":"metrics/"}
metrics.search.type="local"
metrics.search.params={"bucket":"", "path":"src/test/resources/metrics/"}
metrics.period.format.day="MMM dd EEE"
metrics.period.format.month="MMM YYYY"
metrics.period.format.year="YYYY"
metrics.creation.es.url="http://localhost:9200"
metrics.creation.es.indexes="compositesearch"
metrics.dialcode.es.indexes="dialcodemetrics"
metrics.dialcode.request.limit=5
metrics.time.interval.min=30
cache.refresh.time.interval.min=5

# Data Exhaust API
data_exhaust.list.limit="100"
data_exhaust.retry.limit="3"
data_exhaust.dataset.list=["eks-consumption-raw", "eks-consumption-summary", "eks-consumption-metrics","eks-creation-raw", "eks-creation-summary", "eks-creation-metrics"]
data_exhaust.dataset.default="eks-consumption-raw"
data_exhaust.output_format="json"
data_exhaust.bucket="telemetry-data-store"

dataset.request.search.limit=10
dataset.request.search.filters=["dataset", "requestedDate", "status", "channel"]

default.consumption.app.id="no_value"
default.channel.id="in.ekstep"
default.creation.app.id="no_value"

postgres.db="postgres"
postgres.url="jdbc:postgresql://localhost:5432/"
postgres.user="postgres"
postgres.pass="postgres"
postgres.table_name="consumer_channel_mapping"
postgres.table.geo_location_city.name="geo_location_city"
postgres.table.geo_location_city_ipv4.name="geo_location_city_ipv4"
postgres.table.report_config.name="report_config"
postgres.table.job_request.name="job_request"
postgres.table.experiment_definition.name="experiment_definition"
postgres.table.dataset_metadata.name="dataset_metadata"
postgres.table.device_profile.name="device_profile"

channel {
  data_exhaust {
    whitelisted.consumers=["trusted-consumer"]
    expiryMins = 30
    dataset {
      default {
        bucket = "ekstep-dev-data-store"
        basePrefix = "data-exhaust/"
      }
      raw {
        bucket = "ekstep-dev-data-store"
        basePrefix = "data-exhaust/"
      }
      summary {
        bucket = "ekstep-dev-data-store"
        basePrefix = "data-exhaust/"
      }
      summary-rollup {
        bucket = "ekstep-dev-data-store"
        basePrefix = "data-exhaust/"
      }
    }
  }
}

public {
  data_exhaust {
    dataset {
      default {
        bucket = "ekstep-dev-data-store"
        basePrefix = "data-exhaust/"
      }
    }
  }
}


storage-service.request-signature-version="AWS4-HMAC-SHA256"
s3service.region="ap-south-1"

cloud_storage_type="azure"
storage.key.config="azure_storage_key"
storage.secret.config="azure_storage_secret"
public.storage.key.config="azure_storage_key"
public.storage.secret.config="azure_storage_secret"

#redis.host=__redis_host__
redis.host="localhost"
redis.port=6379
#redis.port=__redis_port__
redis.connection.max=2
redis.connection.idle.max=2
redis.connection.idle.min=1
redis.connection.minEvictableIdleTimeSeconds=120
redis.connection.timeBetweenEvictionRunsSeconds=300
redis.experimentIndex=10
redis.deviceIndex=2

elasticsearch.host="localhost"
elasticsearch.port=9200
elasticsearch.searchExperiment.index="experiment"
elasticsearch.searchExperiment.fieldWeight="{\"userId\":3.0, \"deviceId\":2.0, \"url\":1.0 }"
elasticsearch.searchExperiment.matchQueryScore=9.0
deviceRegisterAPI.experiment.enable=true
experimentService.redisEmptyValueExpirySeconds=1000

device-register-actor-dispatcher {
  type = "Dispatcher"
  executor = "fork-join-executor"
  fork-join-executor {
    # The parallelism factor is used to determine thread pool size using the
    # following formula: ceil(available processors * factor). Resulting size
    # is then bounded by the parallelism-min and parallelism-max values.
    parallelism-factor = 3.0

    # Min number of threads to cap factor-based parallelism number to
    parallelism-min = 2

    # Max number of threads to cap factor-based parallelism number to
    parallelism-max = 4
  }
  # Throughput for default Dispatcher, set to 1 for as fair as possible
  throughput = 1
}

device-profile-actor-dispatcher {
  type = "Dispatcher"
  executor = "thread-pool-executor"
  thread-pool-executor {
    fixed-pool-size = 8
  }
  throughput = 1
}

experiment-actor {
  type = "Dispatcher"
  executor = "fork-join-executor"
  fork-join-executor {
    # The parallelism factor is used to determine thread pool size using the
    # following formula: ceil(available processors * factor). Resulting size
    # is then bounded by the parallelism-min and parallelism-max values.
    parallelism-factor = 3.0

    # Min number of threads to cap factor-based parallelism number to
    parallelism-min = 8

    # Max number of threads to cap factor-based parallelism number to
    parallelism-max = 16
  }
  # Throughput for default Dispatcher, set to 1 for as fair as possible
  throughput = 1
}

druid.coordinator.host="http://localhost:8081/"
druid.healthcheck.url="druid/coordinator/v1/loadstatus"

# for only testing uploads to blob store
azure_storage_key=""
azure_storage_secret=""
kafka.broker.list="localhost:9092"
kafka.device.register.topic=dev.events.deviceprofile
kafka.metrics.event.topic=dev.pipeline_metrics

user.profile.url="https://dev.sunbirded.org/api/user/v2/read/"
org.search.url="https://dev.sunbirded.org/api/org/v1/search"
standard.dataexhaust.roles=["ORG_ADMIN","REPORT_ADMIN"]
ondemand.dataexhaust.roles=["ORG_ADMIN","REPORT_ADMIN","COURSE_ADMIN"]
dataexhaust.super.admin.channel=sunbird

cdn.host="https://cdn.abc.com/ekstep-dev-data-store"
public.data_exhaust.datasets=["summary-rollup"]
public.data_exhaust.expiryMonths=2
public.data_exhaust.max.interval.days=30