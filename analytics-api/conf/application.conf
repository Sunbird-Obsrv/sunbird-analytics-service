
# This is the main configuration file for the application.
# ~~~~~

# Secret key
# ~~~~~
# The secret key is used to secure cryptographics functions.
# If you deploy your application to several instances be sure to use the same key!
play.http.secret.key="DFHdR74?tDvH2n`DAqk:RBl6NfkoO5tNYblTRmf3ZLcDIp5@oVjJM^ypkDOf2`<A"


# The application languages
# ~~~~~
application.langs="en"


# Test Configurations
cassandra.service.embedded.enable=false
cassandra.cql_path="../../platform-scripts/database/data.cql"
cassandra.service.embedded.connection.port=9142

spark.cassandra.connection.host="127.0.0.1"
spark.cassandra.connection.port=9042
cassandra.keyspace_prefix="local_"
cassandra.hierarchy_store_prefix="dev_"

# Content to vec configurations
content2vec.content_service_url="https://dev.ekstep.in/api/learning"
recommendation.train_model_job="src/test/resources/run-job.sh device-recos &"


# Recommendations configurations
service.search.url="https://dev.open-sunbird.org/api/composite"
service.search.path="/v1/search"
service.search.requestbody="""{"request":{"filters":{"objectType":["Content"],"contentType":["Story","Worksheet","Collection","Game"],"status":["Live"]},"limit":1000}}"""
service.search.limit="10"

recommendation.enable=true
recommendation.limit="10"
recommendation.surprise_find.enable=true
recommendation.surprise_find.index="15"
recommendation.surprise_find.serendipity_factor="20"


dataproduct.scripts_path=src/test/resources

# Metrics API configuration
#metrics.search.type="s3"
#metrics.search.params={"bucket":"ekstep-dev-data-store", "path":"metrics/"}
metrics.search.type="local"
metrics.search.params={"bucket":"", "path":"test/resources/metrics/"}
metrics.period.format.day="MMM dd EEE"
metrics.period.format.month="MMM YYYY"
metrics.period.format.year="YYYY"


# Data Exhaust API
data_exhaust.list.limit="100"
data_exhaust.retry.limit="3"
data_exhaust.dataset.list=["eks-consumption-raw", "eks-consumption-summary", "eks-consumption-metrics","eks-creation-raw", "eks-creation-summary", "eks-creation-metrics"]
data_exhaust.dataset.default="eks-consumption-raw"
data_exhaust.output_format="json"



# Log4j Kafka appender config
log4j.appender.kafka.enable="false"
log4j.appender.kafka.broker_host="localhost:9092"
log4j.appender.kafka.topic="sandbox.telemetry.backend"

play.modules.enabled+="modules.ActorInjector"
play.filters.enabled+="filter.RequestInterceptor"
play.filters.enabled+="play.filters.hosts.AllowedHostsFilter"
play.filters.disabled += "play.filters.csrf.CSRFFilter"

device-register-controller-dispatcher {
  type = "Dispatcher"
  executor = "fork-join-executor"
  fork-join-executor {
    # The parallelism factor is used to determine thread pool size using the
    # following formula: ceil(available processors * factor). Resulting size
    # is then bounded by the parallelism-min and parallelism-max values.
    parallelism-factor = 3.0

    # Min number of threads to cap factor-based parallelism number to
    parallelism-min = 8

    # Max number of threads to cap factor-based parallelism number to
    parallelism-max = 16
  }
  # Throughput for default Dispatcher, set to 1 for as fair as possible
  throughput = 1
}

device-register-actor-dispatcher {
  type = "Dispatcher"
  executor = "fork-join-executor"
  fork-join-executor {
    # The parallelism factor is used to determine thread pool size using the
    # following formula: ceil(available processors * factor). Resulting size
    # is then bounded by the parallelism-min and parallelism-max values.
    parallelism-factor = 3.0

    # Min number of threads to cap factor-based parallelism number to
    parallelism-min = 8

    # Max number of threads to cap factor-based parallelism number to
    parallelism-max = 16
  }
  # Throughput for default Dispatcher, set to 1 for as fair as possible
  throughput = 1
}

device-profile-actor-dispatcher {
  type = "Dispatcher"
  executor = "fork-join-executor"
  fork-join-executor {
    # The parallelism factor is used to determine thread pool size using the
    # following formula: ceil(available processors * factor). Resulting size
    # is then bounded by the parallelism-min and parallelism-max values.
    parallelism-factor = 3.0

    # Min number of threads to cap factor-based parallelism number to
    parallelism-min = 8

    # Max number of threads to cap factor-based parallelism number to
    parallelism-max = 16
  }
  # Throughput for default Dispatcher, set to 1 for as fair as possible
  throughput = 1
}

experiment-actor {
  type = "Dispatcher"
  executor = "fork-join-executor"
  fork-join-executor {
    # The parallelism factor is used to determine thread pool size using the
    # following formula: ceil(available processors * factor). Resulting size
    # is then bounded by the parallelism-min and parallelism-max values.
    parallelism-factor = 3.0

    # Min number of threads to cap factor-based parallelism number to
    parallelism-min = 8

    # Max number of threads to cap factor-based parallelism number to
    parallelism-max = 16
  }
  # Throughput for default Dispatcher, set to 1 for as fair as possible
  throughput = 1
}

report-actor {
  type = "Dispatcher"
  executor = "fork-join-executor"
  fork-join-executor {
    # The parallelism factor is used to determine thread pool size using the
    # following formula: ceil(available processors * factor). Resulting size
    # is then bounded by the parallelism-min and parallelism-max values.
    parallelism-factor = 3.0

    # Min number of threads to cap factor-based parallelism number to
    parallelism-min = 8

    # Max number of threads to cap factor-based parallelism number to
    parallelism-max = 16
  }
  # Throughput for default Dispatcher, set to 1 for as fair as possible
  throughput = 1
}


default-dispatcher {
  executor = "fork-join-executor"
  fork-join-executor {
    # The parallelism factor is used to determine thread pool size using the
    # following formula: ceil(available processors * factor). Resulting size
    # is then bounded by the parallelism-min and parallelism-max values.
    parallelism-factor = 3.0

    # Min number of threads to cap factor-based parallelism number to
    parallelism-min = 8

    # Max number of threads to cap factor-based parallelism number to
    parallelism-max = 16
  }
  # Throughput for default Dispatcher, set to 1 for as fair as possible
  throughput = 1
}

#AKKA Configuration
akka {
  actor {
  	deployment {
        /job-service-actor {
          router = smallest-mailbox-pool
          nr-of-instances = 2
        }
        /druid-health-actor {
          router = smallest-mailbox-pool
          nr-of-instances = 2
        }
        /client-log-actor {
          router = smallest-mailbox-pool
          nr-of-instances = 2
        }
        /device-register-actor {
          router = smallest-mailbox-pool
          dispatcher = device-register-actor-dispatcher
          nr-of-instances = 2
        }
        /device-profile-actor {
          router = smallest-mailbox-pool
          dispatcher = device-profile-actor-dispatcher
          nr-of-instances = 2
        }
        /experiment-actor {
          router = smallest-mailbox-pool
          nr-of-instances = 2
        }
        /report-actor {
          router = smallest-mailbox-pool
          nr-of-instances = 2
        }
    }
  }
}

#Netty Configuration
play.server {

  # The server provider class name
  provider = "play.core.server.NettyServerProvider"

  netty {

    # The number of event loop threads. 0 means let Netty decide, which by default will select 2 times the number of
    # available processors.
    eventLoopThreads = 30

    # Whether the Netty wire should be logged
    log.wire = true
    
    # The transport to use, either jdk or native.
    # Native socket transport has higher performance and produces less garbage but are only available on linux 
    transport = "jdk"
  }
}

# play.modules.enabled+="MetricsModule"
#play.modules.enabled+="com.kenshoo.play.metrics.PlayModule"
play.modules.enabled+="modules.ActorInjector"
akka.loglevel = DEBUG

# body parser
play.http.parser.maxMemoryBuffer=10M

# app & channel id

default.consumption.app.id="no_value"
default.channel.id="in.ekstep"
default.creation.app.id="no_value"

elasticsearch.service.endpoint="http://localhost:9200"
elasticsearch.index.compositesearch.name="compositesearch"
elasticsearch.index.dialcodemetrics.name="dialcodemetrics"
metrics.dialcodemetrics.request.limit=1000

org.search.api.url="https://dev.sunbirded.org/api/org/v1/search"
org.search.api.key="org-search-api-key"

postgres.db="analytics"
postgres.url="jdbc:postgresql://localhost:5432/"
postgres.user="analytics"
postgres.pass="analytics"
postgres.table_name="consumer_channel_mapping"
postgres.table.geo_location_city.name="geo_location_city"
postgres.table.geo_location_city_ipv4.name="geo_location_city_ipv4"
postgres.table.report_config.name="report_config"

default.channel="in.ekstep"

channel.data_exhaust.bucket="ekstep-dev-data-store"
channel.data_exhaust.basePrefix="channel-exhaust/"
channel.data_exhaust.expiryMins=30
dataexhaust.authorization_check=true

storage-service.request-signature-version="AWS4-HMAC-SHA256"
s3service.region="ap-south-1"

application.env="local"

metrics.time.interval.min=30
cache.refresh.time.interval.min=5

#redis.host=__redis_host__
redis.host="localhost"
redis.port=6379
#redis.port=__redis_port__
redis.connection.max=100
redis.connection.idle.max=2
redis.connection.idle.min=1
redis.connection.minEvictableIdleTimeSeconds=120
redis.connection.timeBetweenEvictionRunsSeconds=300
redis.connection.testOnBorrow=false
redis.connection.testOnReturn=false
redis.experimentIndex=10
redis.deviceIndex=2

elasticsearch.host="localhost"
elasticsearch.port=9200
elasticsearch.searchExperiment.index="experiment"
elasticsearch.searchExperiment.fieldWeight="{\"userId\":3.0, \"deviceId\":3.0, \"url\":3.0 }"
elasticsearch.searchExperiment.matchQueryScore=9.0
deviceRegisterAPI.experiment.enable=false
experimentService.redisEmptyValueExpirySeconds=86400

druid.coordinator.host="http://localhost:8081/"
druid.healthcheck.url="druid/coordinator/v1/loadstatus"

cloud_storage_type="azure"

kafka.broker.list="localhost:9092"
kafka.device.register.topic=dev.events.deviceprofile
kafka.metrics.event.topic=dev.pipeline_metrics

device.api.enable.debug.log=true
